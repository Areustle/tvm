/*!
 *  Copyright (c) 2019 by Contributors
 * \file tvm_mixture.cc
 * \brief Add tvm compute primitives to relay
 */

#include <tvm/relay/op.h>
#include <tvm/relay/attrs/ie_mixture.h>
#include <tvm/ir_operator.h>
#include <tvm/ir.h>
#include <vector>
#include "../op_common.h"
#include "../../../op/op_util.h"

namespace tvm {
namespace relay {

// relay.parsed_tvm_op
bool ParsedTVMOpRel(const Array<Type>& types,
                    int num_inputs,
                    const Attrs& attrs,
                    const TypeReporter& reporter) {
  const auto* tuple_type = ExpectType<TupleTypeNode>(types[0]);
  if (tuple_type == nullptr) { return false; }

  const auto* param = attrs.as<ParsedTVMOpAttrs>();
  if (param->outputs.size() == 1) {
    reporter->Assign(types[1], TensorTypeNode::make(param->outputs[0]->shape,
                                                    param->outputs[0]->dtype));
  } else {
    Array<Type> tuple_fileds;
    for (size_t i = 0; i < param->outputs.size(); ++i) {
      tuple_fileds.push_back(TensorTypeNode::make(param->outputs[i]->shape,
                                                  param->outputs[i]->dtype));
    }
    reporter->Assign(types[1], TupleTypeNode::make(tuple_fileds));
  }
  return true;
}

Array<Tensor> ParsedTVMOpCompute(const Attrs& attrs,
                                 const Array<Tensor>& inputs,
                                 const Type& out_type,
                                 const Target& target) {
  const auto* param = attrs.as<ParsedTVMOpAttrs>();

  std::unordered_map<Tensor, Tensor> placeholder_to_inputs;
  CHECK_EQ(inputs.size(), param->inputs.size());
  for (size_t i = 0; i < inputs.size(); ++i) {
    placeholder_to_inputs[param->inputs[i]] = inputs[i];
  }

  return tvm::op::ReplaceTensorRecursively(param->outputs,
                                           placeholder_to_inputs);
}

RELAY_REGISTER_OP("parsed_tvm_op")
    .describe(R"code(The tvm operator generated by parsing relay index expression.
)code" TVM_ADD_FILELINE)
.set_attrs_type_key("relay.attrs.ParsedTVMOpAttrs")
.set_num_inputs(1)
.add_argument("input", "Tuple", "The input tensors captured by the index expression.")
.set_support_level(5)
.add_type_rel("parse_tvm_op", ParsedTVMOpRel)
.set_attr<TOpPattern>("TOpPattern", kOpaque)
.set_attr<FTVMCompute>("FTVMCompute", ParsedTVMOpCompute)
.set_attr<FTVMSchedule>("FTVMSchedule",
  [](const Attrs& attrs, const Array<Tensor>& outs, const Target& target) {
    Array<tvm::Operation> out_ops;
    for (auto t : outs)
      out_ops.push_back(t->op);
    return create_schedule(out_ops);
  });

// relay.compute
bool ComputeRel(const Array<Type>& types,
                int num_inputs,
                const Attrs& attrs,
                const TypeReporter& reporter) {
  const ComputeAttrs* params = attrs.as<ComputeAttrs>();
  const FuncTypeNode* func_type = ExpectType<FuncTypeNode>(types[0]);
  if (func_type == nullptr) { return false; }

  const TensorTypeNode *ret_type = ExpectType<TensorTypeNode>(func_type->ret_type);
  if (ret_type == nullptr) { return false; }

  CHECK_EQ(ret_type->shape.size(), 0) << "Only accept scalar expression";

  reporter->Assign(types[num_inputs], TensorTypeNode::make(params->shape, ret_type->dtype));
  return true;
}

//Array<Expr> ComputeGradient(const Expr& orig_call, const Expr& output_grad) {
//  const CallNode *call = orig_call.as<CallNode>();
//  CHECK(call != nullptr);
//
//  auto attrs = make_node<ComputeGradAttrs>();
//  attrs->original_call = GetRef<Call>(call);
//
//  Array<Expr> args_in_tuple = call->args;
//  args_in_tuple.push_back(output_grad);
//  auto grad_call = CallNode::make(Op::Get("compute_grad"),
//                                  Array<Expr>{TupleNode::make(args_in_tuple)},
//                                  Attrs(attrs));
//  Array<Expr> res;
//  for (size_t i = 0; i < call->args.size(); ++i) {
//    res.push_back(TupleGetItemNode::make(grad_call, i));
//  }
//  return res;
//}

// Handler to create a call to the compute op used by front-end FFI
Expr MakeCompute(Expr expr, Array<IndexExpr> shape, Array<IndexExpr> reduction) {
  auto attrs = make_node<ComputeAttrs>();
  attrs->shape = std::move(shape);
  attrs->reduction = std::move(reduction);
  static const Op& op = Op::Get("compute");
  return CallNode::make(op, {expr}, Attrs(attrs), {});
}

TVM_REGISTER_API("relay.op._make.compute")
.set_body([](const TVMArgs& args, TVMRetValue* rv) {
runtime::detail::unpack_call<Expr, 3>(MakeCompute, args, rv);
});

RELAY_REGISTER_OP("compute")
.describe(R"code(Compute a new tensor with an index expression
)code" TVM_ADD_FILELINE)
.set_attrs_type_key("relay.attrs.ComputeAttrs")
.set_num_inputs(1)
.add_argument("expr", "Function", "The index expression.")
.set_support_level(5)
.add_type_rel("compute", ComputeRel);
//.set_attr<FPrimalGradient >("FPrimalGradient", ComputeGradient);

// relay.SUM relay.MAX relay.MIN
bool IEReductionOpRel(const Array<Type>& types,
                     int num_inputs,
                     const Attrs& attrs,
                     const TypeReporter& reporter) {
  const auto* tensor_type = ExpectType<TensorTypeNode>(types[0]);
  if (tensor_type == nullptr) { return false; }

  CHECK_EQ(tensor_type->shape.size(), 0);
  reporter->Assign(types[1], GetRef<TensorType>(tensor_type));
  return true;
}

#define IE_REDUCTION_OP(name)                                             \
Expr MakeIEReduction##name(Expr expr) {                                   \
  static const Op& op = Op::Get(#name);                                   \
  return CallNode::make(op, {expr}, Attrs(nullptr), {});                  \
}                                                                         \
                                                                          \
TVM_REGISTER_API("relay.op._make." #name)                                 \
.set_body([](const TVMArgs& args, TVMRetValue* rv) {                      \
  runtime::detail::unpack_call<Expr, 1>(MakeIEReduction##name, args, rv); \
});                                                                       \
                                                                          \
RELAY_REGISTER_OP(#name)                                                  \
    .describe(R"code(The " #name " reduction in index expression language \
)code" TVM_ADD_FILELINE)                                                  \
.set_num_inputs(1)                                                        \
.add_argument("expr", "Expr", "The source expression over axis.")         \
.set_support_level(5)                                                     \
.add_type_rel(#name, IEReductionOpRel);

IE_REDUCTION_OP(SUM);
IE_REDUCTION_OP(MAX);
IE_REDUCTION_OP(MIN);

} // namespace relay
} // namespace tvm